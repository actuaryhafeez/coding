{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression example 1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/actuaryhafeez/coding/blob/master/Logistic_Regression_example_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_nVCcT9CuXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vEhxuz5DUIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7e31f34f-31cb-47f4-a86c-3a1d23328769"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sZlN8C7REfR",
        "colab_type": "text"
      },
      "source": [
        "# Understanding Logistic Regression in Python\n",
        "Learn about Logistic Regression, its basic properties, and build a machine learning model on a real-world application in Python.\n",
        "\n",
        "Classification techniques are an essential part of machine learning and data mining applications. Approximately 70% of problems in Data Science are classification problems. There are lots of classification problems that are available, but the logistics regression is common and is a useful regression method for solving the binary classification problem. \n",
        "\n",
        "Another category of classification is Multinomial classification, which handles the issues where multiple classes are present in the target variable. For example, IRIS dataset a very famous example of multi-class classification. Other examples are classifying article/blog/document category.\n",
        "\n",
        "Logistic Regression can be used for various classification problems such as spam detection. Diabetes prediction, if a given customer will purchase a particular product or will they churn another competitor, whether the user will click on a given advertisement link or not, and many more examples are in the bucket.\n",
        "\n",
        "Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem. Its basic fundamental concepts are also constructive in deep learning. Logistic regression describes and estimates the relationship between one dependent binary variable and independent variables.\n",
        "\n",
        "In this tutorial, you will learn the following things in Logistic Regression:\n",
        "\n",
        "- Introduction to Logistic Regression\n",
        "- Linear Regression Vs. Logistic Regression\n",
        "- Maximum Likelihood Estimation Vs. Ordinary Least Square Method\n",
        "- How do Logistic Regression works?\n",
        "- Model building in Scikit-learn\n",
        "- Model Evaluation using Confusion Matrix.\n",
        "- Advantages and Disadvantages of Logistic Regression\n",
        "\n",
        "# Logistic Regression\n",
        "Logistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature. **Dichotomous** means there are only two possible classes. For example, it can be used for cancer detection problems. It computes the probability of an event occurrence.\n",
        "\n",
        "It is a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function.\n",
        "\n",
        "**Linear Regression Equation:**\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281880/image1_ga8gze.png)\n",
        "\n",
        "\n",
        "Where, y is dependent variable and x1, x2 ... and Xn are explanatory variables.\n",
        "\n",
        "Sigmoid Function:\n",
        "\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281880/image2_kwxquj.png)\n",
        "\n",
        "\n",
        "Apply Sigmoid function on linear regression:\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281880/image3_qldafx.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh3Tt1HoSg9V",
        "colab_type": "text"
      },
      "source": [
        "# Properties of Logistic Regression:\n",
        "\n",
        "- The dependent variable in logistic regression follows Bernoulli Distribution.\n",
        "- Estimation is done through maximum likelihood.\n",
        "- No R Square, Model fitness is calculated through Concordance, KS-Statistics.\n",
        "\n",
        "# Linear Regression Vs. Logistic Regression\n",
        "Linear regression gives you a continuous output, but logistic regression provides a constant output. An example of the continuous output is house price and stock price. Example's of the discrete output is predicting whether a patient has cancer or not, predicting whether the customer will churn. Linear regression is estimated using Ordinary Least Squares (OLS) while logistic regression is estimated using Maximum Likelihood Estimation (MLE) approach.\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281070/linear_vs_logistic_regression_edxw03.png)\n",
        "\n",
        "# Maximum Likelihood Estimation Vs. Least Square Method\n",
        "The MLE is a \"likelihood\" maximization method, while OLS is a distance-minimizing approximation method. Maximizing the likelihood function determines the parameters that are most likely to produce the observed data. From a statistical point of view, MLE sets the mean and variance as parameters in determining the specific parametric values for a given model. This set of parameters can be used for predicting the data needed in a normal distribution.\n",
        "\n",
        "Ordinary Least squares estimates are computed by fitting a regression line on given data points that has the minimum sum of the squared deviations (least square error). Both are used to estimate the parameters of a linear regression model. MLE assumes a joint probability mass function, while OLS doesn't require any stochastic assumptions for minimizing distance.\n",
        "\n",
        "# Sigmoid Function\n",
        "The sigmoid function, also called logistic function gives an ‘S’ shaped curve that can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO. The outputcannotFor example: If the output is 0.75, we can say in terms of probability as: There is a 75 percent chance that patient will suffer from cancer.\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281880/image4_gw5mmv.png)\n",
        "\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281070/sigmoid2_lv4c7i.png)\n",
        "\n",
        "\n",
        "\n",
        "#Types of Logistic Regression\n",
        "Types of Logistic Regression:\n",
        "\n",
        "- Binary Logistic Regression: The target variable has only two possible outcomes such as Spam or Not Spam, Cancer or No Cancer.\n",
        "- Multinomial Logistic Regression: The target variable has three or more nominal categories such as predicting the type of Wine.\n",
        "- Ordinal Logistic Regression: the target variable has three or more ordinal categories such as restaurant or product rating from 1 to 5.\n",
        "\n",
        "#Model building in Scikit-learn\n",
        "Let's build the diabetes prediction model.\n",
        "\n",
        "Here, you are going to predict diabetes using Logistic Regression Classifier.\n",
        "\n",
        "Let's first load the required Pima Indian Diabetes dataset using the pandas' read CSV function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJeYuHJjTuXB",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pBcGB2wDXag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pandas\n",
        "import pandas as pd\n",
        "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
        "# load dataset\n",
        "pima = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/data_sets/pima-indians-diabetes-database.csv\", skiprows = 1, header=None, names=col_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yYwi4ijT71Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "29d7e9a5-ce13-4104-dfee-75de88d281e0"
      },
      "source": [
        "pima.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pregnant</th>\n",
              "      <th>glucose</th>\n",
              "      <th>bp</th>\n",
              "      <th>skin</th>\n",
              "      <th>insulin</th>\n",
              "      <th>bmi</th>\n",
              "      <th>pedigree</th>\n",
              "      <th>age</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pregnant  glucose  bp  skin  insulin   bmi  pedigree  age  label\n",
              "0         6      148  72    35        0  33.6     0.627   50      1\n",
              "1         1       85  66    29        0  26.6     0.351   31      0\n",
              "2         8      183  64     0        0  23.3     0.672   32      1\n",
              "3         1       89  66    23       94  28.1     0.167   21      0\n",
              "4         0      137  40    35      168  43.1     2.288   33      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QQciPeURn5",
        "colab_type": "text"
      },
      "source": [
        "# Selecting Feature\n",
        "Here, you need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubDwiLtzT8_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split dataset in features and target variable\n",
        "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.label # Target variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu3njLR9UZz3",
        "colab_type": "text"
      },
      "source": [
        "# Splitting Data\n",
        "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
        "\n",
        "Let's split dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1jn-C3kUV1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split X and y into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umrwcFeSVD2j",
        "colab_type": "text"
      },
      "source": [
        "Here, the Dataset is broken into two parts in a ratio of 75:25. It means 75% data will be used for model training and 25% for model testing.\n",
        "\n",
        "# Model Development and Prediction\n",
        "First, import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function.\n",
        "\n",
        "Then, fit your model on the train set using fit() and perform prediction on the test set using predict()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXCQg44DViTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the class\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate the model (using the default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train,y_train)\n",
        "\n",
        "#\n",
        "y_pred=logreg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjGiyk4bVs1B",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation using Confusion Matrix\n",
        "A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2Wv4FCdUf3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4fe0dfca-228e-4011-87d6-d91eac9c535e"
      },
      "source": [
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[119,  11],\n",
              "       [ 26,  36]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoDzd9TFV5Rv",
        "colab_type": "text"
      },
      "source": [
        "Here, you can see the confusion matrix in the form of the array object. The dimension of this matrix is 2*2 because this model is binary classification. You have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. In the output, 119 and 36 are actual predictions, and 26 and 11 are incorrect predictions.\n",
        "\n",
        "# Visualizing Confusion Matrix using Heatmap\n",
        "Let's visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn.\n",
        "\n",
        "Here, you will visualize the confusion matrix using Heatmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fha4D8sNVybW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import required modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrcC_kmYWM0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "33c6cacc-c9d8-4ca4-b9b6-8aff620612a0"
      },
      "source": [
        "class_names=[0,1] # name  of classes\n",
        "fig, ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "# create heatmap\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 257.44, 'Predicted label')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAE0CAYAAABuNDcxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZRBJkiplEEmSKmUQSZIqZRBJkir1f57bfQKWMLdMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKe91RysWTPc",
        "colab_type": "text"
      },
      "source": [
        "# Confusion Matrix Evaluation Metrics\n",
        "Let's evaluate the model using model evaluation metrics such as accuracy, precision, and recall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWHbyaXyWNbn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d6a6203b-a548-4449-c28b-05fed1012c80"
      },
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8072916666666666\n",
            "Precision: 0.7659574468085106\n",
            "Recall: 0.5806451612903226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkR_3JbXWhMv",
        "colab_type": "text"
      },
      "source": [
        "Well, you got a classification rate of 80%, considered as good accuracy.\n",
        "\n",
        "Precision: Precision is about being precise, i.e., how accurate your model is. In other words, you can say, when a model makes a prediction, how often it is correct. In your prediction case, when your Logistic Regression model predicted patients are going to suffer from diabetes, that patients have 76% of the time.\n",
        "\n",
        "Recall: If there are patients who have diabetes in the test set and your Logistic Regression model can identify it 58% of the time.\n",
        "\n",
        "# ROC Curve\n",
        "Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9L0gW89Wagf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "12153eb3-1caa-454d-9e0c-8b870ce5b5a6"
      },
      "source": [
        "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP4mDVeeW1xC",
        "colab_type": "text"
      },
      "source": [
        "AUC score for the case is 0.86. AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier.\n",
        "\n",
        "# Advantages\n",
        "Because of its efficient and straightforward nature, doesn't require high computation power, easy to implement, easily interpretable, used widely by data analyst and scientist. Also, it doesn't require scaling of features. Logistic regression provides a probability score for observations.\n",
        "\n",
        "# Disadvantages\n",
        "Logistic regression is not able to handle a large number of categorical features/variables. It is vulnerable to overfitting. Also, can't solve the non-linear problem with the logistic regression that is why it requires a transformation of non-linear features. Logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other.\n",
        "\n",
        "# Conclusion\n",
        "In this tutorial, you covered a lot of details about Logistic Regression. You have learned what the logistic regression is, how to build respective models, how to visualize results and some of the theoretical background information. Also, you covered some basic concepts such as the sigmoid function, maximum likelihood, confusion matrix, ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M_a_86QXZOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}