{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced NLP with spaCy.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/actuaryhafeez/coding/blob/master/Advanced_NLP_with_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuEukQL_yN2N",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started\n",
        "Let's get started and try out spaCy! In this exercise, you'll be able to try out some of the 45+ available languages.\n",
        "\n",
        "This course introduces a lot of new concepts, so if you ever need a quick refresher, download the spaCy Cheat Sheet and keep it handy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmrAWhehyC--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41de4d42-9a25-4884-8045-e2370d951a70"
      },
      "source": [
        "# Import the English language class\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = English()\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"This is a sentence.\")\n",
        "\n",
        "# Print the document text\n",
        "print(doc.text)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a sentence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7agtDCA5yTwk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f21bca97-e015-4d0c-c51d-918575255df0"
      },
      "source": [
        "# Import the German language class\n",
        "from spacy.lang.de import German\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = German()\n",
        "\n",
        "# Process a text (this is German for: \"Kind regards!\")\n",
        "doc = nlp(\"Liebe Grüße!\")\n",
        "\n",
        "# Print the document text\n",
        "print(doc.text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Liebe Grüße!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pjFZD_1yt3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfa6b1a8-d58f-4b71-c1f2-bb20f7d9d231"
      },
      "source": [
        "from spacy.lang.es import Spanish\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = Spanish()\n",
        "\n",
        "# Process a text (this is Spanish for: \"How are you?\")\n",
        "doc = nlp(\"¿Cómo estás?\")\n",
        "\n",
        "# Print the document text\n",
        "print(doc.text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "¿Cómo estás?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeUnpWmPzFzE",
        "colab_type": "text"
      },
      "source": [
        "# Documents, spans and tokens\n",
        "When you call nlp on a string, spaCy first tokenizes the text and creates a document object. In this exercise, you'll learn more about the Doc, as well as its views Token and Span."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYa9TPEuy-nk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff82e30e-06b7-4734-a5fc-6829591eb630"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
        "\n",
        "# Select the first token\n",
        "first_token = doc[0]\n",
        "\n",
        "# Print the first token's text\n",
        "print(first_token.text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc84LXS-zR4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a067f71c-041a-4444-f643-39332cf723cb"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
        "\n",
        "# A slice of the Doc for \"tree kangaroos\"\n",
        "tree_kangaroos = doc[2:4]\n",
        "print(tree_kangaroos.text)\n",
        "\n",
        "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
        "tree_kangaroos_and_narwhals = doc[2:6]\n",
        "print(tree_kangaroos_and_narwhals.text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tree kangaroos\n",
            "tree kangaroos and narwhals\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P2oeD9xzff_",
        "colab_type": "text"
      },
      "source": [
        "# Lexical attributes\n",
        "In this example, you'll use spaCy's Doc and Token objects, and lexical attributes to find percentages in a text. You'll be looking for two subsequent tokens: a number and a percent sign. The English nlp object has already been created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGe1kHdsziWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "20c27d3e-ccaf-4c62-9950-79b2288337f3"
      },
      "source": [
        "# Process the text\n",
        "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n",
        "\n",
        "# Iterate over the tokens in the doc\n",
        "for token in doc:\n",
        "    # Check if the token resembles a number\n",
        "    if token.like_num:\n",
        "        # Get the next token in the document\n",
        "        next_token = doc[token.i + 1]\n",
        "        # Check if the next token's text equals '%'\n",
        "        if next_token.text == '%':\n",
        "            print('Percentage found:', token.text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage found: 60\n",
            "Percentage found: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi1ybYzx0XUO",
        "colab_type": "text"
      },
      "source": [
        "# Loading models\n",
        "Let's start by loading a model. spacy is already imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzy9erUxzmZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf37e5f1-c5d8-4b94-8bcf-64bdae844525"
      },
      "source": [
        "import spacy\n",
        "# Load the 'en_core_web_sm' model – spaCy is already imported\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the document text\n",
        "print(doc.text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndrhi9zl32lG",
        "colab_type": "text"
      },
      "source": [
        "# Predicting linguistic annotations\n",
        "You'll now get to try one of spaCy's pre-trained model packages and see its predictions in action. Feel free to try it out on your own text! The small English model is already available as the variable nlp.\n",
        "\n",
        "To find out what a tag or label means, you can call spacy.explain in the IPython shell. For example: spacy.explain('PROPN') or spacy.explain('GPE')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ9axTa234De",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "668a83e8-40c1-4f18-b9b0-4cff96b3f58d"
      },
      "source": [
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    # This is for formatting only\n",
        "    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It          PRON      nsubj     \n",
            "’s          PROPN     ROOT      \n",
            "official    NOUN      acomp     \n",
            ":           PUNCT     punct     \n",
            "Apple       PROPN     nsubj     \n",
            "is          VERB      ROOT      \n",
            "the         DET       det       \n",
            "first       ADJ       amod      \n",
            "U.S.        PROPN     nmod      \n",
            "public      ADJ       amod      \n",
            "company     NOUN      attr      \n",
            "to          PART      aux       \n",
            "reach       VERB      relcl     \n",
            "a           DET       det       \n",
            "$           SYM       quantmod  \n",
            "1           NUM       compound  \n",
            "trillion    NUM       nummod    \n",
            "market      NOUN      compound  \n",
            "value       NOUN      dobj      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVaAMd1T4Wz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "08db620d-5f36-48cd-ec50-86a401dd47db"
      },
      "source": [
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "    # print the entity text and its label\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "first ORDINAL\n",
            "U.S. GPE\n",
            "$1 trillion MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KI3t--J4oXd",
        "colab_type": "text"
      },
      "source": [
        "# Predicting named entities in context\n",
        "Models are statistical and not always right. Whether their predictions are correct depends on the training data and the text you're processing. Let's take a look at an example. The small English model is available as the variable nlp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5twDNz9n4fVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bedf8932-e5e7-491f-bcd7-961bf28b8056"
      },
      "source": [
        "text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the entities\n",
        "for ent in doc.ents:\n",
        "    # print the entity text and label\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "# Get the span for \"iPhone X\"\n",
        "iphone_x = doc[1:3]\n",
        "\n",
        "# Print the span text\n",
        "print('Missing entity:', iphone_x.text)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "Missing entity: iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1npds_55dqt",
        "colab_type": "text"
      },
      "source": [
        "# Using the Matcher\n",
        "Let's try spaCy's rule-based Matcher. You'll be using the example from the previous exercise and write a pattern that can match the phrase \"iPhone X\" in the text. The nlp object and a processed doc are already available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e3Tv1Mz40C0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42ab1421-bf84-479b-bd48-1368ddd1444a"
      },
      "source": [
        "# Import the Matcher\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Initialize the Matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
        "pattern = [{'TEXT': \"iPhone\"}, \n",
        "           {'TEXT': \"X\"}]\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "matcher.add('IPHONE_X_PATTERN', None, pattern)\n",
        "\n",
        "# Import the Matcher and initialize it with the shared vocabulary\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
        "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "matcher.add('IPHONE_X_PATTERN', None, pattern)\n",
        "\n",
        "# Use the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches: ['iPhone X']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rGaH8Aa6Uew",
        "colab_type": "text"
      },
      "source": [
        "# Writing match patterns\n",
        "In this exercise, you'll practice writing more complex match patterns using different token attributes and operators. A matcher is already initialized and available as the variable matcher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65XCukIL5vaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e07943a0-6f88-4cb5-d72d-fbba550f0570"
      },
      "source": [
        "doc = nlp(\"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\")\n",
        "\n",
        "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
        "pattern = [{'TEXT': 'iOS'}, {'IS_DIGIT': True}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add('IOS_VERSION_PATTERN', None, pattern)\n",
        "matches = matcher(doc)\n",
        "print('Total matches found:', len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print('Match found:', doc[start:end].text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 3\n",
            "Match found: iOS 7\n",
            "Match found: iOS 11\n",
            "Match found: iOS 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2s9Skx46rT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bc91259d-6a53-42a0-a7b8-1b33276d7f8d"
      },
      "source": [
        "doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\")\n",
        "\n",
        "# Write a pattern that matches a form of \"download\" plus proper noun\n",
        "pattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add('DOWNLOAD_THINGS_PATTERN', None, pattern)\n",
        "matches = matcher(doc)\n",
        "print('Total matches found:', len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print('Match found:', doc[start:end].text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 3\n",
            "Match found: downloaded Fortnite\n",
            "Match found: downloading Minecraft\n",
            "Match found: download Winzip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E6uzMLj60Ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "838c6455-27c7-41e8-f5f0-1bf81a2eb176"
      },
      "source": [
        "doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
        "\n",
        "# Write a pattern for adjective plus one or two nouns\n",
        "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add('ADJ_NOUN_PATTERN', None, pattern)\n",
        "matches = matcher(doc)\n",
        "print('Total matches found:', len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print('Match found:', doc[start:end].text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 4\n",
            "Match found: beautiful design\n",
            "Match found: smart search\n",
            "Match found: automatic labels\n",
            "Match found: optional voice responses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik2yW-hM8VO-",
        "colab_type": "text"
      },
      "source": [
        "# Strings to hashes\n",
        "The nlp object has already been created for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cswBOpYgEoFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3c14692b-dc08-478e-8357-e4cbcb52673f"
      },
      "source": [
        "person_hash = nlp.vocab.strings['PERSON']\n",
        "print(person_hash)\n",
        "\n",
        "# Look up the person_hash to get the string\n",
        "person_string = nlp.vocab.strings[person_hash]\n",
        "print(person_string)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "380\n",
            "PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riXcnfSDFhsb",
        "colab_type": "text"
      },
      "source": [
        "# Creating a Doc\n",
        "Let's create some Doc objects from scratch! The nlp object has already been created for you.\n",
        "\n",
        "By the way, if you haven't downloaded it already, check out the spaCy Cheat Sheet. It includes an overview of the most important concepts and methods and might come in handy if you ever need a quick refresher!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YxbVNt98hgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "961d0db7-77bf-425f-964f-1bd125bdfccf"
      },
      "source": [
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Desired text: \"spaCy is cool!\"\n",
        "words = ['spaCy', 'is', 'cool', '!']\n",
        "spaces = [True, True, False, False]\n",
        "\n",
        "# Create a Doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy is cool!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXILI7g_FwK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85557218-8194-44ab-ca66-b61a05b24e31"
      },
      "source": [
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Desired text: \"Go, get started!\"\n",
        "words = ['Go', ',', 'get', 'started', '!']\n",
        "spaces = [False, True, True, False, False]\n",
        "\n",
        "# Create a Doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go, get started!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpV1pTWfF3ux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "961198f9-b31e-4ef2-c150-2275bd8b8868"
      },
      "source": [
        "from spacy.tokens import Doc\n",
        "\n",
        "# Desired text: \"Oh, really?!\"\n",
        "words = ['Oh', ',', 'really', '?', '!']\n",
        "spaces = [False, True, False, False, False]\n",
        "\n",
        "# Create a Doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oh, really?!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CmkmkMTGF3m",
        "colab_type": "text"
      },
      "source": [
        "# Docs, spans and entities from scratch\n",
        "In this exercise, you'll create the Doc and Span objects manually, and update the named entities – just like spaCy does behind the scenes. A shared nlp object has already been created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfmF8jnEF8kB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "694369cd-8ef3-470e-f551-1ad5f2455cb3"
      },
      "source": [
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "words = ['I', 'like', 'David', 'Bowie']\n",
        "spaces = [True, True, True, False]\n",
        "\n",
        "# Create a doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like David Bowie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSRNVDTbGUfM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5e498f1-5366-4eca-bbf5-3a4e84b66767"
      },
      "source": [
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "# Create a doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
        "\n",
        "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
        "span = Span(doc, 2, 4, label='PERSON')\n",
        "print(span.text, span.label_)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "David Bowie PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beG3OSu7GZDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13b67b7c-eb83-4577-fc73-268ecfd7ba6b"
      },
      "source": [
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "# Create a doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
        "\n",
        "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
        "span = Span(doc, 2, 4, label='PERSON')\n",
        "\n",
        "# Add the span to the doc's entities\n",
        "doc.ents = [span]\n",
        "\n",
        "# Print entities' text and labels\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('David Bowie', 'PERSON')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v-oEyEhG7Mu",
        "colab_type": "text"
      },
      "source": [
        "# Data structures best practices\n",
        "The code in this example is trying to analyze a text and collect all proper nouns. If the token following the proper noun is a verb, it should also be extracted. A doc object has already been created.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg5d40m7HBxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all tokens and part-of-speech tags\n",
        "pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "for index, pos in enumerate(pos_tags):\n",
        "    # Check if the current token is a proper noun\n",
        "    if pos == 'PROPN':\n",
        "        # Check if the next token is a verb\n",
        "        if pos_tags[index + 1] == 'VERB':\n",
        "            print('Found a verb after a proper noun!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH4TbtASHCh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterate over the tokens\n",
        "for token in doc:\n",
        "    # Check if the current token is a proper noun\n",
        "    if token.pos_ == 'PROPN':\n",
        "        # Check if the next token is a verb\n",
        "        if doc[token.i + 1].pos_ == 'VERB':\n",
        "            print('Found a verb after a proper noun!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUv8A1PVILKC",
        "colab_type": "text"
      },
      "source": [
        "# Inspecting word vectors\n",
        "In this exercise, you'll use a larger English model, which includes around 20.000 word vectors. Because vectors take a little longer to load, we're using a slightly compressed version of it than the one you can download with spaCy. The model is already pre-installed, and spacy has already been imported for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unIGhchMJJ0g",
        "colab_type": "text"
      },
      "source": [
        "# Comparing similarities\n",
        "In this exercise, you'll be using spaCy's similarity methods to compare Doc, Token and Span objects and get similarity scores. The medium English model is already available as the nlp object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvfA_R_VJLhR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "11b896b1-1ee9-430b-b6d5-ac08b7012ed7"
      },
      "source": [
        "\n",
        "## Comparing similarities\n",
        "\n",
        "doc1 = nlp(\"It's a warm summer day\")\n",
        "doc2 = nlp(\"It's sunny outside\")\n",
        "\n",
        "# Get the similarity of doc1 and doc2\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5566261254871249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OeVZz2mJYrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7478f887-a267-49e9-8e3d-879949aeffdd"
      },
      "source": [
        "doc = nlp(\"TV and books\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "# Get the similarity of the tokens \"TV\" and \"books\" \n",
        "similarity = token1.similarity(token2)\n",
        "print(similarity)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3083113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4WI2w7lJgmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "10f96a4f-5f3a-447c-abfc-31ff2257927c"
      },
      "source": [
        "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
        "\n",
        "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
        "span1 = doc[3:5]\n",
        "span2 = doc[12:15]\n",
        "\n",
        "# Get the similarity of the spans\n",
        "similarity = span1.similarity(span2)\n",
        "print(similarity)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7244181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-cwtrRKgCW",
        "colab_type": "text"
      },
      "source": [
        "# Debugging patterns (2)\n",
        "Both patterns in this exercise contain mistakes and won't match as expected. Can you fix them?\n",
        "\n",
        "The nlp and a doc have already been created for you. If you get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWJBSlAJJkmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the match patterns\n",
        "pattern1 = [{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
        "pattern2 = [{'LOWER': 'ad'}, {'TEXT': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]\n",
        "\n",
        "# Initialize the Matcher and add the patterns\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add('PATTERN1', None, pattern1)\n",
        "matcher.add('PATTERN2', None, pattern2)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Print pattern string name and text of matched span\n",
        "    print(doc.vocab.strings[match_id], doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl4Bb8BJK6nd",
        "colab_type": "text"
      },
      "source": [
        "# Efficient phrase matching\n",
        "Sometimes it's more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world.\n",
        "\n",
        "We already have a list of countries, so let's use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES. The nlp object and a test doc have already been created and the doc.text has been printed to the shell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV8tDgfVLKr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COUNTRIES= ['Afghanistan', 'Åland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia (Plurinational State of)', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'United States Minor Outlying Islands', 'Virgin Islands (British)', 'Virgin Islands (U.S.)', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cabo Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo (Democratic Republic of the)', 'Cook Islands', 'Costa Rica', 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', \"Côte d'Ivoire\", 'Iran (Islamic Republic of)', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia (the former Yugoslav Republic of)', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia (Federated States of)', 'Moldova (Republic of)', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \"Korea (Democratic People's Republic of)\", 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine, State of', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of Kosovo', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Korea (Republic of)', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela (Bolivarian Republic of)', 'Viet Nam', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb54GAdCKsz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "264700af-2c57-4641-abc1-e86e7523dd75"
      },
      "source": [
        "# Import the PhraseMatcher and initialize it\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "# Create pattern Doc objects and add them to the matcher\n",
        "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
        "patterns = list(nlp.pipe(COUNTRIES))\n",
        "matcher.add('COUNTRY', None, *patterns)\n",
        "\n",
        "# Call the matcher on the test document and print the result\n",
        "matches = matcher(doc)\n",
        "print([doc[start:end] for match_id, start, end in matches])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C51oRZPLdAI",
        "colab_type": "text"
      },
      "source": [
        "# Extracting countries and relationships\n",
        "In the previous exercise, you wrote a script using spaCy's PhraseMatcher to find country names in text. Let's use that country matcher on a longer text, analyze the syntax and update the document's entities with the matched countries. The nlp object has already been created.\n",
        "\n",
        "The text is available as the variable text, the PhraseMatcher with the country patterns as the variable matcher. The Span class has already been imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MTtYiiOLv3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'After the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more missions in ten years than it had in the previous four decades.Between 1988 and 2000, the number of adopted Security Council resolutions more than doubled, and the peacekeeping budget increased more than tenfold. The UN negotiated an end to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and oversaw democratic elections in post-apartheid South Africa and post-Khmer Rouge Cambodia. In 1991, the UN authorized a US-led coalition that repulsed the Iraqi invasion of Kuwait. Brian Urquhart, Under-Secretary-General from 1971 to 1985, later described the hopes raised by these successes as a \"false renaissance\" for the organization, given the more troubled missions that followed. Though the UN Charter had been written primarily to prevent aggression by one nation against another, in the early 1990s the UN faced a number of simultaneous, serious crises within nations such as Somalia, Haiti, Mozambique, and the former Yugoslavia. The UN mission in Somalia was widely viewed as a failure after the US withdrawal following casualties in the Battle of Mogadishu, and the UN mission to Bosnia faced \"worldwide ridicule\" for its indecisive and confused mission in the face of ethnic cleansing. In 1994, the UN Assistance Mission for Rwanda failed to intervene in the Rwandan genocide amid indecision in the Security Council. Beginning in the last decades of the Cold War, American and European critics of the UN condemned the organization for perceived mismanagement and corruption. In 1984, the US President, Ronald Reagan, withdrew his nation\\'s funding from UNESCO (the United Nations Educational, Scientific and Cultural Organization, founded 1946) over allegations of mismanagement, followed by Britain and Singapore. Boutros Boutros-Ghali, Secretary-General from 1992 to 1996, initiated a reform of the Secretariat, reducing the size of the organization somewhat. His successor, Kofi Annan (1997–2006), initiated further management reforms in the face of threats from the United States to withhold its UN dues. In the late 1990s and 2000s, international interventions authorized by the UN took a wider variety of forms. The UN mission in the Sierra Leone Civil War of 1991–2002 was supplemented by British Royal Marines, and the invasion of Afghanistan in 2001 was overseen by NATO. In 2003, the United States invaded Iraq despite failing to pass a UN Security Council resolution for authorization, prompting a new round of questioning of the organization\\'s effectiveness. Under the eighth Secretary-General, Ban Ki-moon, the UN has intervened with peacekeepers in crises including the War in Darfur in Sudan and the Kivu conflict in the Democratic Republic of Congo and sent observers and chemical weapons inspectors to the Syrian Civil War. In 2013, an internal review of UN actions in the final battles of the Sri Lankan Civil War in 2009 concluded that the organization had suffered \"systemic failure\". One hundred and one UN personnel died in the 2010 Haiti earthquake, the worst loss of life in the organization\\'s history. The Millennium Summit was held in 2000 to discuss the UN\\'s role in the 21st century. The three day meeting was the largest gathering of world leaders in history, and culminated in the adoption by all member states of the Millennium Development Goals (MDGs), a commitment to achieve international development in areas such as poverty reduction, gender equality, and public health. Progress towards these goals, which were to be met by 2015, was ultimately uneven. The 2005 World Summit reaffirmed the UN\\'s focus on promoting development, peacekeeping, human rights, and global security. The Sustainable Development Goals were launched in 2015 to succeed the Millennium Development Goals. In addition to addressing global challenges, the UN has sought to improve its accountability and democratic legitimacy by engaging more with civil society and fostering a global constituency. In an effort to enhance transparency, in 2016 the organization held its first public debate between candidates for Secretary-General. On 1 January 2017, Portuguese diplomat António Guterres, who previously served as UN High Commissioner for Refugees, became the ninth Secretary-General. Guterres has highlighted several key goals for his administration, including an emphasis on diplomacy for preventing conflicts, more effective peacekeeping efforts, and streamlining the organization to be more responsive and versatile to global needs.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhrMP-d9K-4s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "60db5ec3-4e4c-4f8e-d5ec-ee613e2dd5e5"
      },
      "source": [
        "# Create a doc and find matches in it\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Create a Span with the label for \"GPE\"\n",
        "    span = Span(doc, start, end, label='GPE')\n",
        "\n",
        "    # Overwrite the doc.ents and add the span\n",
        "    doc.ents = list(doc.ents) + [span]\n",
        "    \n",
        "# Print the entities in the document\n",
        "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-f6c53b640dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Overwrite the doc.ents and add the span\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Print the entities in the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(74, 75, 'GPE')' and '(74, 75, 'GPE')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGdfVv1zLmkD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "17025a4e-746c-4fac-912f-a6e96d5d9c0a"
      },
      "source": [
        "# Create a doc and find matches in it\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Create a Span with the label for \"GPE\" and overwrite the doc.ents\n",
        "    span = Span(doc, start, end, label='GPE')\n",
        "    doc.ents = list(doc.ents) + [span]\n",
        "    \n",
        "    # Get the span's root head token\n",
        "    span_root_head = span.root.head\n",
        "    # Print the text of the span root's head token and the span text\n",
        "    print(span_root_head.text, '-->', span.text)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-11d481ec9180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create a Span with the label for \"GPE\" and overwrite the doc.ents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GPE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Get the span's root head token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(74, 75, 'GPE')' and '(74, 75, 'GPE')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1ZX6XljMQh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}